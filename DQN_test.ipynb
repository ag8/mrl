{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1mWarning: Failed to import private modules.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mrl.configs.make_discrete_agents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing env!\n",
      "Initializing env!\n"
     ]
    }
   ],
   "source": [
    "config = make_dqn_agent(args=Namespace(env='InvertedPendulum-v2',\n",
    "                                       tb='',\n",
    "                                       parent_folder='/tmp/mrl',\n",
    "                                       layers=(32, 1),\n",
    "                                       num_envs=1,\n",
    "                                       device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu',\n",
       " 'gamma': 0.99,\n",
       " 'qvalue_lr': 0.001,\n",
       " 'qvalue_weight_decay': 0.0,\n",
       " 'optimize_every': 2,\n",
       " 'batch_size': 1000,\n",
       " 'warm_up': 10000,\n",
       " 'initial_explore': 10000,\n",
       " 'grad_norm_clipping': -1,\n",
       " 'grad_value_clipping': -1,\n",
       " 'random_action_prob': <mrl.utils.schedule.LinearSchedule at 0x7f0e7bc184e0>,\n",
       " 'target_network_update_frac': 0.005,\n",
       " 'target_network_update_freq': 2,\n",
       " 'clip_target_range': (-inf, inf),\n",
       " 'go_eexplore': 0.1,\n",
       " 'go_reset_percent': 0.025,\n",
       " 'overshoot_goal_percent': False,\n",
       " 'direct_overshoots': False,\n",
       " 'dg_score_multiplier': 1.0,\n",
       " 'cutoff_success_threshold': (0.3, 0.7),\n",
       " 'initial_cutoff': -3,\n",
       " 'double_q': False,\n",
       " 'activ': 'gelu',\n",
       " 'curiosity_beta': -3.0,\n",
       " 'seed': 0,\n",
       " 'replay_size': 1000000,\n",
       " 'num_envs': 1,\n",
       " 'log_every': 5000,\n",
       " 'use_qvalue_target': False,\n",
       " 'her': 'futureactual_2_2',\n",
       " 'prioritized_mode': 'none',\n",
       " 'future_warm_up': 20000,\n",
       " 'sparse_reward_shaping': 0.0,\n",
       " 'n_step_returns': 1,\n",
       " 'slot_based_state': False,\n",
       " 'num_eval_envs': 1,\n",
       " 'annotationdict': {'device': 'torch device (cpu or gpu)',\n",
       "  'gamma': 'discount factor',\n",
       "  'qvalue_lr': 'Q-value learning rate',\n",
       "  'qvalue_weight_decay': 'weight decay to apply to qvalue',\n",
       "  'optimize_every': 'how often optimize is called, in terms of environment steps',\n",
       "  'batch_size': 'batch size for training the Q-values',\n",
       "  'warm_up': 'minimum steps in replay buffer needed to optimize',\n",
       "  'initial_explore': 'whether to act randomly during warmup',\n",
       "  'grad_norm_clipping': 'gradient norm clipping (implemented as backward hook)',\n",
       "  'grad_value_clipping': 'gradient value clipping',\n",
       "  'random_action_prob': 'Epsilon decay schedule',\n",
       "  'target_network_update_frac': 'polyak averaging coefficient for target networks',\n",
       "  'target_network_update_freq': 'how often to update target networks; NOTE: TD3 uses this too!',\n",
       "  'clip_target_range': 'q/value targets are clipped to this range',\n",
       "  'go_eexplore': 'epsilon exploration bonus from each point of go explore, when using intrinsic curiosity',\n",
       "  'go_reset_percent': 'probability to reset epsiode early for each point of go explore, when using intrinsic curiosity',\n",
       "  'overshoot_goal_percent': 'if using instrinsic goals, should goal be overshot on success?',\n",
       "  'direct_overshoots': 'if using overshooting, should it be directed in a straight line?',\n",
       "  'dg_score_multiplier': 'if using instrinsic goals, score multiplier for goal candidates that are in DG distribution',\n",
       "  'initial_cutoff': 'initial (and minimum) cutoff for intrinsic goal curiosity',\n",
       "  'double_q': 'Use Double DQN or not. Default: False',\n",
       "  'activ': 'activation to use for hidden layers in networks',\n",
       "  'curiosity_beta': 'beta to use for curiosity_alpha module',\n",
       "  'seed': 'random seed',\n",
       "  'replay_size': 'maximum size of replay buffer',\n",
       "  'num_envs': 'number of parallel envs to run',\n",
       "  'log_every': 'how often to log things',\n",
       "  'use_qvalue_target': 'if true, use target network to act in the environment',\n",
       "  'her': 'strategy to use for hindsight experience replay',\n",
       "  'prioritized_mode': 'buffer prioritization strategy',\n",
       "  'future_warm_up': 'minimum steps in replay buffer needed to stop doing ONLY future sampling',\n",
       "  'sparse_reward_shaping': 'coefficient of euclidean distance reward shaping in sparse goal envs',\n",
       "  'n_step_returns': 'if using n-step returns, how many steps?',\n",
       "  'slot_based_state': 'if state is organized by slot; i.e., [batch_size, num_slots, slot_feats]',\n",
       "  'num_eval_envs': 'number of parallel eval envs to run'},\n",
       " 'parent_folder': '/tmp/mrl',\n",
       " 'other_args': {'env': 'InvertedPendulum-v2',\n",
       "  'tb': '1597702057.237494',\n",
       "  'layers': (32, 1),\n",
       "  'prefix': 'dqn'},\n",
       " 'agent_name': 'dqn_env-InvertedPendulum-v2_seed0_tb-1597702057.237494',\n",
       " 'module_train': <mrl.modules.train.StandardTrain at 0x7f0e7bc18518>,\n",
       " 'module_eval': <mrl.modules.eval.EpisodicEval at 0x7f0e7bc18550>,\n",
       " 'module_policy': <mrl.algorithms.discrete_off_policy.QValuePolicy at 0x7f0e7bc18588>,\n",
       " 'module_logger': <mrl.modules.logging.Logger at 0x7f0e7bc185c0>,\n",
       " 'module_state_normalizer': <mrl.modules.normalizer.Normalizer at 0x7f0e7bc18630>,\n",
       " 'module_replay': <mrl.replays.online_her_buffer.OnlineHERBuffer at 0x7f0e7bc18668>,\n",
       " 'module_action_noise': None,\n",
       " 'module_algorithm': <mrl.algorithms.discrete_off_policy.DQN at 0x7f0e7bc186a0>,\n",
       " 'module_train_env': <mrl.modules.env.EnvModule at 0x7f0e7bc186d8>,\n",
       " 'module_eval_env': <mrl.modules.env.EnvModule at 0x7f0edb72c588>,\n",
       " 'module_actor': <mrl.modules.model.PytorchModel at 0x7f0e73b08fd0>,\n",
       " 'module_critic': <mrl.modules.model.PytorchModel at 0x7f0e73a6e400>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Agent is missing module qvalue",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-679b5c8b1ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_to_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mrl/mrl/agent_base.py\u001b[0m in \u001b[0;36mconfig_to_agent\u001b[0;34m(config_dict)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mrl/mrl/agent_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module_list, config)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mrl/mrl/agent_base.py\u001b[0m in \u001b[0;36m_register_module\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify_agent_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mrl/mrl/agent_base.py\u001b[0m in \u001b[0;36mverify_agent_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequired_agent_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Agent is missing module {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Agent is missing module qvalue"
     ]
    }
   ],
   "source": [
    "agent = mrl.config_to_agent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eval(num_episodes=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
