{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1mWarning: Failed to import private modules.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from mrl.configs.make_continuous_agents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing env!\n",
      "Initializing env!\n"
     ]
    }
   ],
   "source": [
    "config = make_ddpg_agent(args=Namespace(env='InvertedPendulum-v2',\n",
    "                                    tb='',\n",
    "                                    parent_folder='/tmp/mrl',\n",
    "                                    layers=(32, 1),\n",
    "                                    num_envs=1,\n",
    "                                    device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'device': 'cpu',\n",
       " 'gamma': 0.99,\n",
       " 'actor_lr': 0.001,\n",
       " 'critic_lr': 0.001,\n",
       " 'actor_weight_decay': 0.0,\n",
       " 'action_l2_regularization': 0.01,\n",
       " 'critic_weight_decay': 0.0,\n",
       " 'optimize_every': 2,\n",
       " 'batch_size': 2000,\n",
       " 'warm_up': 10000,\n",
       " 'initial_explore': 10000,\n",
       " 'grad_norm_clipping': -1.0,\n",
       " 'grad_value_clipping': -1.0,\n",
       " 'target_network_update_frac': 0.005,\n",
       " 'target_network_update_freq': 1,\n",
       " 'clip_target_range': (-inf, inf),\n",
       " 'td3_noise': 0.1,\n",
       " 'td3_noise_clip': 0.3,\n",
       " 'td3_delay': 2,\n",
       " 'entropy_coef': 0.2,\n",
       " 'policy_opt_noise': 0.0,\n",
       " 'action_noise': 0.1,\n",
       " 'eexplore': 0.0,\n",
       " 'go_eexplore': 0.1,\n",
       " 'go_reset_percent': 0.0,\n",
       " 'overshoot_goal_percent': 0.0,\n",
       " 'direct_overshoots': False,\n",
       " 'dg_score_multiplier': 1.0,\n",
       " 'cutoff_success_threshold': (0.3, 0.7),\n",
       " 'initial_cutoff': -3,\n",
       " 'activ': 'gelu',\n",
       " 'curiosity_beta': -3.0,\n",
       " 'sigma_l2_regularization': 0.0,\n",
       " 'seed': 0,\n",
       " 'replay_size': 1000000,\n",
       " 'save_replay_buf': False,\n",
       " 'num_envs': 1,\n",
       " 'num_eval_envs': 1,\n",
       " 'log_every': 5000,\n",
       " 'varied_action_noise': False,\n",
       " 'use_actor_target': False,\n",
       " 'her': 'futureactual_2_2',\n",
       " 'prioritized_mode': 'none',\n",
       " 'future_warm_up': 25000,\n",
       " 'sparse_reward_shaping': 0.0,\n",
       " 'n_step_returns': 1,\n",
       " 'slot_based_state': False,\n",
       " 'annotationdict': {'device': 'torch device (cpu or gpu)',\n",
       "  'gamma': 'discount factor',\n",
       "  'actor_lr': 'actor learning rate',\n",
       "  'critic_lr': 'critic learning rate',\n",
       "  'actor_weight_decay': 'weight decay to apply to actor',\n",
       "  'action_l2_regularization': 'l2 penalty for action norm',\n",
       "  'critic_weight_decay': 'weight decay to apply to critic',\n",
       "  'optimize_every': 'how often optimize is called, in terms of environment steps',\n",
       "  'batch_size': 'batch size for training the actors/critics',\n",
       "  'warm_up': 'minimum steps in replay buffer needed to optimize',\n",
       "  'initial_explore': 'steps that actor acts randomly for at beginning of training',\n",
       "  'grad_norm_clipping': 'gradient norm clipping',\n",
       "  'grad_value_clipping': 'gradient value clipping',\n",
       "  'target_network_update_frac': 'polyak averaging coefficient for target networks',\n",
       "  'target_network_update_freq': 'how often to update target networks; NOTE: TD3 uses this too!',\n",
       "  'clip_target_range': 'q/value targets are clipped to this range',\n",
       "  'td3_noise': 'noise added to next step actions in td3',\n",
       "  'td3_noise_clip': 'amount to which next step noise in td3 is clipped',\n",
       "  'td3_delay': 'how often the actor is trained, in terms of critic training steps, in td3',\n",
       "  'entropy_coef': 'Entropy regularization coefficient for SAC',\n",
       "  'policy_opt_noise': 'how much policy noise to add to actor optimization',\n",
       "  'action_noise': 'maximum std of action noise',\n",
       "  'eexplore': 'how often to do completely random exploration (overrides action noise)',\n",
       "  'go_eexplore': 'epsilon exploration bonus from each point of go explore, when using intrinsic curiosity',\n",
       "  'go_reset_percent': 'probability to reset episode early for each point of go explore, when using intrinsic curiosity',\n",
       "  'overshoot_goal_percent': 'if using instrinsic FIRST VISIT goals, should goal be overshot on success?',\n",
       "  'direct_overshoots': 'if using overshooting, should it be directed in a straight line?',\n",
       "  'dg_score_multiplier': 'if using instrinsic goals, score multiplier for goal candidates that are in DG distribution',\n",
       "  'initial_cutoff': 'initial (and minimum) cutoff for intrinsic goal curiosity',\n",
       "  'activ': 'activation to use for hidden layers in networks',\n",
       "  'curiosity_beta': 'beta to use for curiosity_alpha module',\n",
       "  'sigma_l2_regularization': 'l2 regularization on sigma critics log variance',\n",
       "  'seed': 'random seed',\n",
       "  'replay_size': 'maximum size of replay buffer',\n",
       "  'save_replay_buf': 'save replay buffer checkpoint during training?',\n",
       "  'num_envs': 'number of parallel envs to run',\n",
       "  'num_eval_envs': 'number of parallel eval envs to run',\n",
       "  'log_every': 'how often to log things',\n",
       "  'varied_action_noise': 'if true, action noise for each env in vecenv is interpolated between 0 and action noise',\n",
       "  'use_actor_target': 'if true, use actor target network to act in the environment',\n",
       "  'her': 'strategy to use for hindsight experience replay',\n",
       "  'prioritized_mode': 'buffer prioritization strategy',\n",
       "  'future_warm_up': 'minimum steps in replay buffer needed to stop doing ONLY future sampling',\n",
       "  'sparse_reward_shaping': 'coefficient of euclidean distance reward shaping in sparse goal envs',\n",
       "  'n_step_returns': 'if using n-step returns, how many steps?',\n",
       "  'slot_based_state': 'if state is organized by slot; i.e., [batch_size, num_slots, slot_feats]'},\n",
       " 'parent_folder': '/tmp/mrl',\n",
       " 'other_args': {'env': 'InvertedPendulum-v2',\n",
       "  'tb': '1597685532.028292',\n",
       "  'layers': (32, 1),\n",
       "  'prefix': 'ddpg'},\n",
       " 'agent_name': 'ddpg_env-InvertedPendulum-v2_seed0_tb-1597685532.028292',\n",
       " 'module_train': <mrl.modules.train.StandardTrain at 0x7f7f5b07f4e0>,\n",
       " 'module_eval': <mrl.modules.eval.EpisodicEval at 0x7f7f5b07f518>,\n",
       " 'module_policy': <mrl.algorithms.continuous_off_policy.ActorPolicy at 0x7f7f5b07f550>,\n",
       " 'module_logger': <mrl.modules.logging.Logger at 0x7f7f5b07f588>,\n",
       " 'module_state_normalizer': <mrl.modules.normalizer.Normalizer at 0x7f7f5b07f5f8>,\n",
       " 'module_replay': <mrl.replays.online_her_buffer.OnlineHERBuffer at 0x7f7f5b07f630>,\n",
       " 'module_action_noise': <mrl.modules.action_noise.ContinuousActionNoise at 0x7f7f5b07f6a0>,\n",
       " 'module_algorithm': <mrl.algorithms.continuous_off_policy.DDPG at 0x7f7f5b07f6d8>,\n",
       " 'module_train_env': <mrl.modules.env.EnvModule at 0x7f7f5b07f710>,\n",
       " 'module_eval_env': <mrl.modules.env.EnvModule at 0x7f7fbab9a588>,\n",
       " 'module_actor': <mrl.modules.model.PytorchModel at 0x7f7f52f0d080>,\n",
       " 'module_critic': <mrl.modules.model.PytorchModel at 0x7f7f52ed9470>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35;1m\n",
      "Agent folder:\u001b[0m\n",
      "/tmp/mrl/ddpg_env-InvertedPendulum-v2_seed0_tb-1597685532.028292\n",
      "\u001b[36;1m\n",
      "Saving config:\u001b[0m\n",
      "{\n",
      "    \"action_l2_regularization\":\t0.01,\n",
      "    \"action_noise\":\t0.1,\n",
      "    \"activ\":\t\"gelu\",\n",
      "    \"actor_lr\":\t0.001,\n",
      "    \"actor_weight_decay\":\t0.0,\n",
      "    \"agent_name\":\t\"ddpg_env-InvertedPendulum-v2_seed0_tb-1597685532.028292\",\n",
      "    \"annotationdict\":\t{\n",
      "        \"action_l2_regularization\":\t\"l2 penalty for action norm\",\n",
      "        \"action_noise\":\t\"maximum std of action noise\",\n",
      "        \"activ\":\t\"activation to use for hidden layers in networks\",\n",
      "        \"actor_lr\":\t\"actor learning rate\",\n",
      "        \"actor_weight_decay\":\t\"weight decay to apply to actor\",\n",
      "        \"batch_size\":\t\"batch size for training the actors/critics\",\n",
      "        \"clip_target_range\":\t\"q/value targets are clipped to this range\",\n",
      "        \"critic_lr\":\t\"critic learning rate\",\n",
      "        \"critic_weight_decay\":\t\"weight decay to apply to critic\",\n",
      "        \"curiosity_beta\":\t\"beta to use for curiosity_alpha module\",\n",
      "        \"device\":\t\"torch device (cpu or gpu)\",\n",
      "        \"dg_score_multiplier\":\t\"if using instrinsic goals, score multiplier for goal candidates that are in DG distribution\",\n",
      "        \"direct_overshoots\":\t\"if using overshooting, should it be directed in a straight line?\",\n",
      "        \"eexplore\":\t\"how often to do completely random exploration (overrides action noise)\",\n",
      "        \"entropy_coef\":\t\"Entropy regularization coefficient for SAC\",\n",
      "        \"future_warm_up\":\t\"minimum steps in replay buffer needed to stop doing ONLY future sampling\",\n",
      "        \"gamma\":\t\"discount factor\",\n",
      "        \"go_eexplore\":\t\"epsilon exploration bonus from each point of go explore, when using intrinsic curiosity\",\n",
      "        \"go_reset_percent\":\t\"probability to reset episode early for each point of go explore, when using intrinsic curiosity\",\n",
      "        \"grad_norm_clipping\":\t\"gradient norm clipping\",\n",
      "        \"grad_value_clipping\":\t\"gradient value clipping\",\n",
      "        \"her\":\t\"strategy to use for hindsight experience replay\",\n",
      "        \"initial_cutoff\":\t\"initial (and minimum) cutoff for intrinsic goal curiosity\",\n",
      "        \"initial_explore\":\t\"steps that actor acts randomly for at beginning of training\",\n",
      "        \"log_every\":\t\"how often to log things\",\n",
      "        \"n_step_returns\":\t\"if using n-step returns, how many steps?\",\n",
      "        \"num_envs\":\t\"number of parallel envs to run\",\n",
      "        \"num_eval_envs\":\t\"number of parallel eval envs to run\",\n",
      "        \"optimize_every\":\t\"how often optimize is called, in terms of environment steps\",\n",
      "        \"overshoot_goal_percent\":\t\"if using instrinsic FIRST VISIT goals, should goal be overshot on success?\",\n",
      "        \"policy_opt_noise\":\t\"how much policy noise to add to actor optimization\",\n",
      "        \"prioritized_mode\":\t\"buffer prioritization strategy\",\n",
      "        \"replay_size\":\t\"maximum size of replay buffer\",\n",
      "        \"save_replay_buf\":\t\"save replay buffer checkpoint during training?\",\n",
      "        \"seed\":\t\"random seed\",\n",
      "        \"sigma_l2_regularization\":\t\"l2 regularization on sigma critics log variance\",\n",
      "        \"slot_based_state\":\t\"if state is organized by slot; i.e., [batch_size, num_slots, slot_feats]\",\n",
      "        \"sparse_reward_shaping\":\t\"coefficient of euclidean distance reward shaping in sparse goal envs\",\n",
      "        \"target_network_update_frac\":\t\"polyak averaging coefficient for target networks\",\n",
      "        \"target_network_update_freq\":\t\"how often to update target networks; NOTE: TD3 uses this too!\",\n",
      "        \"td3_delay\":\t\"how often the actor is trained, in terms of critic training steps, in td3\",\n",
      "        \"td3_noise\":\t\"noise added to next step actions in td3\",\n",
      "        \"td3_noise_clip\":\t\"amount to which next step noise in td3 is clipped\",\n",
      "        \"use_actor_target\":\t\"if true, use actor target network to act in the environment\",\n",
      "        \"varied_action_noise\":\t\"if true, action noise for each env in vecenv is interpolated between 0 and action noise\",\n",
      "        \"warm_up\":\t\"minimum steps in replay buffer needed to optimize\"\n",
      "    },\n",
      "    \"batch_size\":\t2000,\n",
      "    \"clip_target_range\":\t[\n",
      "        -Infinity,\n",
      "        Infinity\n",
      "    ],\n",
      "    \"critic_lr\":\t0.001,\n",
      "    \"critic_weight_decay\":\t0.0,\n",
      "    \"curiosity_beta\":\t-3.0,\n",
      "    \"cutoff_success_threshold\":\t[\n",
      "        0.3,\n",
      "        0.7\n",
      "    ],\n",
      "    \"device\":\t\"cpu\",\n",
      "    \"dg_score_multiplier\":\t1.0,\n",
      "    \"direct_overshoots\":\tfalse,\n",
      "    \"eexplore\":\t0.0,\n",
      "    \"entropy_coef\":\t0.2,\n",
      "    \"env_steps\":\t0,\n",
      "    \"future_warm_up\":\t25000,\n",
      "    \"gamma\":\t0.99,\n",
      "    \"go_eexplore\":\t0.1,\n",
      "    \"go_reset_percent\":\t0.0,\n",
      "    \"grad_norm_clipping\":\t-1.0,\n",
      "    \"grad_value_clipping\":\t-1.0,\n",
      "    \"her\":\t\"futureactual_2_2\",\n",
      "    \"initial_cutoff\":\t-3,\n",
      "    \"initial_explore\":\t10000,\n",
      "    \"log_every\":\t5000,\n",
      "    \"module_action_noise\":\t\"{'kwargs': {'std': <mrl.utils.schedule.ConstantSchedule object at 0x7f7f5b07f668>}, 'args': (), 'random_process_cls': <class 'mrl.utils.random_process.GaussianProcess'>, 'self': <mrl.modules.action_noise.ContinuousActionNoise object at 0x7f7f5b07f6a0>}\",\n",
      "    \"module_actor\":\t\"{'model_fn': <function make_ddpg_agent.<locals>.<lambda> at 0x7f7f52ece488>, 'name': 'actor', 'self': <mrl.modules.model.PytorchModel object at 0x7f7f52f0d080>}\",\n",
      "    \"module_algorithm\":\t\"{'self': <mrl.algorithms.continuous_off_policy.DDPG object at 0x7f7f5b07f6d8>}\",\n",
      "    \"module_critic\":\t\"{'model_fn': <function make_ddpg_agent.<locals>.<lambda> at 0x7f7f52ed50d0>, 'name': 'critic', 'self': <mrl.modules.model.PytorchModel object at 0x7f7f52ed9470>}\",\n",
      "    \"module_env\":\t\"{'name': None, 'num_envs': 1, 'self': <mrl.modules.env.EnvModule object at 0x7f7f5b07f710>, 'seed': 0, 'episode_life': True, 'env': <function make_ddpg_agent.<locals>.<lambda> at 0x7f7f5b0807b8>}\",\n",
      "    \"module_eval\":\t\"{'self': <mrl.modules.eval.EpisodicEval object at 0x7f7f5b07f518>}\",\n",
      "    \"module_eval_env\":\t\"{'name': 'eval_env', 'num_envs': 1, 'self': <mrl.modules.env.EnvModule object at 0x7f7fbab9a588>, 'seed': 1138, 'episode_life': True, 'env': <function make_ddpg_agent.<locals>.<lambda> at 0x7f7f5b0807b8>}\",\n",
      "    \"module_logger\":\t\"{'average_every': 100, 'self': <mrl.modules.logging.Logger object at 0x7f7f5b07f588>}\",\n",
      "    \"module_policy\":\t\"{'self': <mrl.algorithms.continuous_off_policy.ActorPolicy object at 0x7f7f5b07f550>}\",\n",
      "    \"module_replay_buffer\":\t\"{'module_name': 'replay_buffer', 'self': <mrl.replays.online_her_buffer.OnlineHERBuffer object at 0x7f7f5b07f630>}\",\n",
      "    \"module_state_normalizer\":\t\"{'normalizer': <mrl.modules.normalizer.MeanStdNormalizer object at 0x7f7f5b07f5c0>, 'self': <mrl.modules.normalizer.Normalizer object at 0x7f7f5b07f5f8>}\",\n",
      "    \"module_train\":\t\"{'self': <mrl.modules.train.StandardTrain object at 0x7f7f5b07f4e0>}\",\n",
      "    \"n_step_returns\":\t1,\n",
      "    \"num_envs\":\t1,\n",
      "    \"num_eval_envs\":\t1,\n",
      "    \"opt_steps\":\t0,\n",
      "    \"optimize_every\":\t2,\n",
      "    \"other_args\":\t{\n",
      "        \"env\":\t\"InvertedPendulum-v2\",\n",
      "        \"layers\":\t[\n",
      "            32,\n",
      "            1\n",
      "        ],\n",
      "        \"prefix\":\t\"ddpg\",\n",
      "        \"tb\":\t\"1597685532.028292\"\n",
      "    },\n",
      "    \"overshoot_goal_percent\":\t0.0,\n",
      "    \"parent_folder\":\t\"/tmp/mrl\",\n",
      "    \"policy_opt_noise\":\t0.0,\n",
      "    \"prioritized_mode\":\t\"none\",\n",
      "    \"replay_size\":\t1000000,\n",
      "    \"save_replay_buf\":\tfalse,\n",
      "    \"seed\":\t0,\n",
      "    \"sigma_l2_regularization\":\t0.0,\n",
      "    \"slot_based_state\":\tfalse,\n",
      "    \"sparse_reward_shaping\":\t0.0,\n",
      "    \"target_network_update_frac\":\t0.005,\n",
      "    \"target_network_update_freq\":\t1,\n",
      "    \"td3_delay\":\t2,\n",
      "    \"td3_noise\":\t0.1,\n",
      "    \"td3_noise_clip\":\t0.3,\n",
      "    \"use_actor_target\":\tfalse,\n",
      "    \"varied_action_noise\":\tfalse,\n",
      "    \"warm_up\":\t10000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "agent = mrl.config_to_agent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mrl.agent_base.Agent at 0x7f7f5bc5cbe0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewards': [17.0,\n",
       "  17.0,\n",
       "  16.0,\n",
       "  17.0,\n",
       "  16.0,\n",
       "  17.0,\n",
       "  18.0,\n",
       "  18.0,\n",
       "  15.0,\n",
       "  17.0,\n",
       "  18.0,\n",
       "  16.0,\n",
       "  18.0,\n",
       "  16.0,\n",
       "  15.0,\n",
       "  16.0,\n",
       "  16.0,\n",
       "  17.0,\n",
       "  18.0,\n",
       "  15.0],\n",
       " 'steps': [17.0,\n",
       "  17.0,\n",
       "  16.0,\n",
       "  17.0,\n",
       "  16.0,\n",
       "  17.0,\n",
       "  18.0,\n",
       "  18.0,\n",
       "  15.0,\n",
       "  17.0,\n",
       "  18.0,\n",
       "  16.0,\n",
       "  18.0,\n",
       "  16.0,\n",
       "  15.0,\n",
       "  16.0,\n",
       "  16.0,\n",
       "  17.0,\n",
       "  18.0,\n",
       "  15.0]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.eval(num_episodes=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
